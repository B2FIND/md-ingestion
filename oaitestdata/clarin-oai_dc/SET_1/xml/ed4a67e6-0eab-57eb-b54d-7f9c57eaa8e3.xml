<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
  <header>
    <identifier>oai:lindat.mff.cuni.cz:11372/LRT-1271</identifier>
    <datestamp>2016-04-06T16:39:49Z</datestamp>
    <setSpec>hdl_11858_00-097C-0000-0007-710A-A</setSpec>
    <setSpec>hdl_11858_00-097C-0000-0007-710B-8</setSpec>
  </header>
  <metadata>
    <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:doc="http://www.lyncode.com/xoai" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:dc="http://purl.org/dc/elements/1.1/" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
      <dc:title>Translation Equivalents Extractor</dc:title>
      <dc:creator>Tufi&#351;, Dan</dc:creator>
      <dc:creator>Ion, Radu</dc:creator>
      <dc:creator>Barbu, Ana-Maria</dc:creator>
      <dc:description>TREQ exploits the knowledge embedded in the parallel corpora and produces a set of&#13;
translation equivalents (a translation lexicon), based on a 1:1 mapping&#13;
hypothesis. The program uses almost no linguistic knowledge, relying on statistical evidence and some simplifying assumptions. &#13;
The extraction process is based on a testing approach. It generates first a list of translation equivalent candidates and then successively extracts the most likely translation equivalence pairs. It does not require a pre-existing bilingual lexicon for the considered languages. Yet, if such a lexicon exists, it can be used to eliminate spurious candidate translation equivalence pairs and thus to speed up the process and increase its accuracy. The algorithm relies on some pre-processing of the bitext: sentence aligner, tokeniser (using [[(http://www.lpl.univaix.fr/projects/multext/MtSeg|MtSeg]]), a collocation extractor (unaware of translation equivalence), POS-tagger, lemmatiser. &#13;
More detailed descriptions are available in the following paper (http://www.racai.ro/~tufis/papers/): &#13;
  -- Dan  Tufi&#351; and Ana-Maria Barbu (2002). Revealing translators knowledge: statistical methods in constructing practical translation lexicons for language and speech processing. In International Journal of Speech Technology, volume 5, pp. 199-209. Kluwer Academic Publishers, November 2002. ISSN 1381-2416. &#13;
  -- Dan Tufi&#351; (2002). A cheap and fast way to build useful translation lexicons. In Proceedings of the 19th International Conference on Computational Linguistics (COLING 2002), pp. 1030-1036, Taipei, Taiwan, August 2002. ISBN 1-55860-894. &#13;
  -- Dan Tufi&#351; and Ana Maria Barbu (2001). Automatic Construction of Translation Lexicons. In V.V.Kluew, C.E. D'Attellis, and N.E. Mastorakis (eds.), Advances in Automation, Multimedia and Video Systems, and Modern Computer Science, pp. 156-161. WSES Press, December 2001. ISSN 1790-5117. &#13;
  -- Dan  Tufi&#351; and Ana Maria Barbu (2001). Extracting Multilingual Lexicons from Parallel Corpora. In Proceedings of the ACH-ALLC conference (ACH-ALLC 2001), New York, USA, June 2001. &#13;
  -- Dan  Tufi&#351; and Ana Maria Barbu (2001). Accurate Automatic Extraction of Translation Equivalents from Parallel Corpora. In Paul Rayson, Andrew Wilson, Tony McEnery, Andrew Hardie, and Shereen Khoja., editors, Proceedings of the Corpus Linguistics 2001 Conference (CL 2001), pp. 581-586, Lancaster, UK, March 2001. Lancaster University, Computing Department. ISBN 1-86220-107-2.</dc:description>
      <dc:date>2014-07-30</dc:date>
      <dc:type>toolService</dc:type>
      <dc:identifier>http://hdl.handle.net/11372/LRT-1271</dc:identifier>
      <dc:format>downloadable_files_count: 0</dc:format>
      <dc:coverage>Romania</dc:coverage>
      <dc:publisher>Research Institute for Artificial Intelligence, Romanian Academy of Sciences</dc:publisher>
    </oai_dc:dc>
  </metadata>
</record>
