<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
  <header>
    <identifier>oai:www.clarin.si:11356/1068</identifier>
    <datestamp>2017-01-20T13:58:56Z</datestamp>
    <setSpec>hdl_11356_1023</setSpec>
    <setSpec>hdl_11356_1024</setSpec>
  </header>
  <metadata>
    <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:doc="http://www.lyncode.com/xoai" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:dc="http://purl.org/dc/elements/1.1/" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
      <dc:title>Dataset of normalised Slovene text KonvNormSl 1.0</dc:title>
      <dc:creator>Ljube&#353;i&#263;, Nikola</dc:creator>
      <dc:creator>Zupan, Katja</dc:creator>
      <dc:creator>Fi&#353;er, Darja</dc:creator>
      <dc:creator>Erjavec, Toma&#382;</dc:creator>
      <dc:subject>word normalisation</dc:subject>
      <dc:subject>historical language</dc:subject>
      <dc:subject>computer-mediated communication</dc:subject>
      <dc:subject>experimental data</dc:subject>
      <dc:subject>manual annotation</dc:subject>
      <dc:description>Data used in the experiments described in:&#13;
&#13;
Nikola Ljube&#353;i&#263;, Katja Zupan, Darja Fi&#353;er and Toma&#382; Erjavec: Normalising Slovene data: historical texts vs. user-generated content. Proceedings of KONVENS 2016, September 19&#8211;21, 2016, Bochum, Germany.&#13;
https://www.linguistics.rub.de/konvens16/pub/19_konvensproc.pdf&#13;
(https://www.linguistics.rub.de/konvens16/)&#13;
&#13;
Data are split into the "token" folder (experiment on normalising individual tokens) and "segment" folder (experiment on normalising whole segments of text, i.e. sentences or tweets). Each experiment folder contains the "train", "dev" and "test" subfolders. Each subfolder contains two files for each sample, the original data (*.orig.txt) and the data with hand-normalised words (*.norm.txt). The files are aligned by lines.&#13;
&#13;
There are four datasets:&#13;
- goo300k-bohoric: historical Slovene, hard case (&lt;1850)&#13;
- goo300k-gaj: historical Slovene, easy case (1850 - 1900)&#13;
- tweet-L3: Slovene tweets, hard case (non-standard language)&#13;
- tweet-L1: Slovene tweets, easy case (mostly standard language)&#13;
&#13;
The goo300k data come from http://hdl.handle.net/11356/1025, while the tweet data originate from the JANES project (http://nl.ijs.si/janes/english/).&#13;
&#13;
The text in the files has been split by inserting spaces between characters, with underscore (_) substituting the space character. Tokens not relevant for normalisation (e.g. URLs, hashtags) have been substituted by the inverted question mark '&#191;' character.</dc:description>
      <dc:date>2016-09-19</dc:date>
      <dc:type>corpus</dc:type>
      <dc:identifier>http://hdl.handle.net/11356/1068</dc:identifier>
      <dc:language>slv</dc:language>
      <dc:rights>Creative Commons - Attribution-ShareAlike 4.0 International (CC BY-SA 4.0)</dc:rights>
      <dc:rights>https://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:rights>PUB</dc:rights>
      <dc:format>text/plain; charset=utf-8</dc:format>
      <dc:format>application/zip</dc:format>
      <dc:format>downloadable_files_count: 1</dc:format>
      <dc:publisher>Jo&#382;ef Stefan Institute</dc:publisher>
    </oai_dc:dc>
  </metadata>
</record>
