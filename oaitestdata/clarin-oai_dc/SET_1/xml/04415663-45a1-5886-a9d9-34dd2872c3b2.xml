<record xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
  <header>
    <identifier>oai:lindat.mff.cuni.cz:11858/00-097C-0000-000D-F696-9</identifier>
    <datestamp>2018-07-02T22:05:50Z</datestamp>
    <setSpec>hdl_11858_00-097C-0000-0001-486F-D</setSpec>
    <setSpec>hdl_11858_00-097C-0000-0001-4877-A</setSpec>
  </header>
  <metadata>
    <oai_dc:dc xmlns:oai_dc="http://www.openarchives.org/OAI/2.0/oai_dc/" xmlns:doc="http://www.lyncode.com/xoai" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:dc="http://purl.org/dc/elements/1.1/" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/oai_dc/ http://www.openarchives.org/OAI/2.0/oai_dc.xsd">
      <dc:title>jusText</dc:title>
      <dc:creator>Pomik&#225;lek, Jan</dc:creator>
      <dc:subject>boilerplate</dc:subject>
      <dc:subject>web documents</dc:subject>
      <dc:subject>text cleaning</dc:subject>
      <dc:subject>boilerplate removal</dc:subject>
      <dc:subject>text corpora</dc:subject>
      <dc:description>jusText is a heuristic based boilerplate removal tool useful for cleaning documents in large textual corpora. The tool has been implemented in Python, licensed under New BSD License and made an open source software (available for download including the source code at http://code.google.com/p/justext/). It is successfully used for cleaning large textual corpora at Natural language processing centre at Faculty of informatics, Masaryk university Brno and it's industry partners. The research leading to this piece of software was published in author's Ph.D. thesis "Removing Boilerplate and Duplicate Content from Web Corpora". The boilerplate removal algorithm is able to remove most of non-grammatical sentences from a web page like navigation, advertisements, tables, short notes and so on. It has been shown it overperforms or at least keeps up with it's competitors (according to comparison with participants of Cleaneval competition in author's Ph.D. thesis). The precise removal of unwanted content and scalability of the algorithm has been demonstrated while building corpora of American Spanish, Arabic, Czech, French, Japanese, Russian, Tajik, and six Turkic languages consisting --- over 20 TB of HTML pages were processed resulting in corpora of 70 billions tokens altogether.</dc:description>
      <dc:date>2011</dc:date>
      <dc:type>toolService</dc:type>
      <dc:identifier>http://hdl.handle.net/11858/00-097C-0000-000D-F696-9</dc:identifier>
      <dc:language>eng</dc:language>
      <dc:rights>Attribution-ShareAlike 3.0 Unported (CC BY-SA 3.0)</dc:rights>
      <dc:rights>http://creativecommons.org/licenses/by-sa/3.0/</dc:rights>
      <dc:rights>PUB</dc:rights>
      <dc:format>application/x-gzip</dc:format>
      <dc:format>text/plain; charset=utf-8</dc:format>
      <dc:format>downloadable_files_count: 1</dc:format>
      <dc:publisher>Masaryk University, NLP Centre</dc:publisher>
      <dc:source>http://code.google.com/p/justext/</dc:source>
    </oai_dc:dc>
  </metadata>
</record>
