{
    "Contact": [
        "University of Sheffield"
    ],
    "DiscHierarchy": [
        "1.4",
        "Humanities",
        "Linguistics"
    ],
    "Discipline": "Linguistics",
    "Format": [
        "text/plain; charset=utf-8",
        "application/x-gzip",
        "downloadable_files_count: 3"
    ],
    "Language": [
        "English",
        "German",
        "Czech",
        "Latvian"
    ],
    "MetaDataAccess": "https://clarin-pl.eu/oai/request?verb=GetRecord&metadataPrefix=oai_dc&identifier=oai:lindat.mff.cuni.cz:11372/LRT-2805",
    "MetadataAccess": [
        "oai:lindat.mff.cuni.cz:11372/LRT-2805"
    ],
    "PID": "http://hdl.handle.net/11372/LRT-2805",
    "PublicationTimestamp": "2018-05-21T11:59:59Z",
    "PublicationYear": [
        "2018"
    ],
    "Publisher": [
        "University of Sheffield"
    ],
    "RelatedIdentifier": [
        "info:eu-repo/grantAgreement/EC/H2020/645452",
        "http://hdl.handle.net/11372/LRT-2135"
    ],
    "ResourceType": [
        "corpus"
    ],
    "Rights": [
        "AGREEMENT ON THE USE OF DATA IN QT21",
        "https://lindat.mff.cuni.cz/repository/xmlui/page/licence-TAUS_QT21",
        "PUB"
    ],
    "author": [
        "Fernandez, Ramon",
        "Martins, Andr\u00e9",
        "Logacheva, Varvara",
        "Specia, Lucia",
        "Blain, Frederic"
    ],
    "fulltext": "oai:lindat.mff.cuni.cz:11372/LRT-2805;2018-05-21T15:23:41Z;hdl_11858_00-097C-0000-0007-710A-A;hdl_11858_00-097C-0000-0007-710B-8;WMT18 Quality Estimation Shared Task Test Data;Specia, Lucia;Logacheva, Varvara;Blain, Frederic;Fernandez, Ramon;Martins, Andr\u00e9;machine translation;quality estimation;machine learning;Test data for the WMT18 QE task. Train data can be downloaded from http://hdl.handle.net/11372/LRT-2619.\r\n\r\nThis shared task will build on its previous six editions to further examine automatic methods for estimating the quality of machine translation output at run-time, without relying on reference translations. We include word-level, phrase-level and sentence-level estimation. All tasks make use of datasets produced from post-editions by professional translators. The datasets are domain-specific (IT and life sciences/pharma domains) and extend from those used previous years with more instances and more languages. One important addition is that this year we also include datasets with neural MT outputs. In addition to advancing the state of the art at all prediction levels, our specific goals are:\r\n\r\nTo study the performance of quality estimation approaches on the output of neural MT systems. We will do so by providing datasets for two language language pairs where the same source segments are translated by both a statistical phrase-based and a neural MT system.\r\n\r\nTo study the predictability of deleted words, i.e. words that are missing in the MT output. TO do so, for the first time we provide data annotated for such errors at training time.\r\n\r\nTo study the effectiveness of explicitly assigned labels for phrases. We will do so by providing a dataset where each phrase in the output of a phrase-based statistical MT system was annotated by human translators.\r\n\r\nTo study the effect of different language pairs. We will do so by providing datasets created in similar ways for four language language pairs.\r\n\r\nTo investigate the utility of detailed information logged during post-editing. We will do so by providing post-editing time, keystrokes, and actual edits.\r\n\r\nMeasure progress over years at all prediction levels. We will do so by using last year's test set for comparative experiments.\r\n\r\nIn-house statistical and neural MT systems were built to produce translations for all tasks. MT system-dependent information can be made available under request. The data is publicly available but since it has been provided by our industry partners it is subject to specific terms and conditions. However, these have no practical implications on the use of this data for research purposes. Participants are allowed to explore any additional data and resources deemed relevant.;2018-05-21;corpus;http://hdl.handle.net/11372/LRT-2805;eng;deu;ces;lav;info:eu-repo/grantAgreement/EC/H2020/645452;http://hdl.handle.net/11372/LRT-2135;AGREEMENT ON THE USE OF DATA IN QT21;https://lindat.mff.cuni.cz/repository/xmlui/page/licence-TAUS_QT21;PUB;application/x-gzip;application/x-gzip;application/x-gzip;text/plain; charset=utf-8;downloadable_files_count: 3;University of Sheffield;http://www.statmt.org/wmt18/quality-estimation-task.html",
    "group": "clarin",
    "groups": [
        {
            "name": "clarin"
        }
    ],
    "name": "68041a03-e765-5d02-af49-38e2446b86a6",
    "notes": [
        "Test data for the WMT18 QE task. Train data can be downloaded from http://hdl.handle.net/11372/LRT-2619.\r\n\r\nThis shared task will build on its previous six editions to further examine automatic methods for estimating the quality of machine translation output at run-time, without relying on reference translations. We include word-level, phrase-level and sentence-level estimation. All tasks make use of datasets produced from post-editions by professional translators. The datasets are domain-specific (IT and life sciences/pharma domains) and extend from those used previous years with more instances and more languages. One important addition is that this year we also include datasets with neural MT outputs. In addition to advancing the state of the art at all prediction levels, our specific goals are:\r\n\r\nTo study the performance of quality estimation approaches on the output of neural MT systems. We will do so by providing datasets for two language language pairs where the same source segments are translated by both a statistical phrase-based and a neural MT system.\r\n\r\nTo study the predictability of deleted words, i.e. words that are missing in the MT output. TO do so, for the first time we provide data annotated for such errors at training time.\r\n\r\nTo study the effectiveness of explicitly assigned labels for phrases. We will do so by providing a dataset where each phrase in the output of a phrase-based statistical MT system was annotated by human translators.\r\n\r\nTo study the effect of different language pairs. We will do so by providing datasets created in similar ways for four language language pairs.\r\n\r\nTo investigate the utility of detailed information logged during post-editing. We will do so by providing post-editing time, keystrokes, and actual edits.\r\n\r\nMeasure progress over years at all prediction levels. We will do so by using last year's test set for comparative experiments.\r\n\r\nIn-house statistical and neural MT systems were built to produce translations for all tasks. MT system-dependent information can be made available under request. The data is publicly available but since it has been provided by our industry partners it is subject to specific terms and conditions. However, these have no practical implications on the use of this data for research purposes. Participants are allowed to explore any additional data and resources deemed relevant."
    ],
    "oai_identifier": [
        "oai:lindat.mff.cuni.cz:11372/LRT-2805"
    ],
    "oai_set": [
        "hdl_11858_00-097C-0000-0007-710A-A",
        "hdl_11858_00-097C-0000-0007-710B-8"
    ],
    "state": "active",
    "tags": [
        {
            "name": "machine translation"
        },
        {
            "name": "quality estimation"
        },
        {
            "name": "machine learning"
        }
    ],
    "title": [
        "WMT18 Quality Estimation Shared Task Test Data"
    ],
    "url": ""
}