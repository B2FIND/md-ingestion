{
    "Contact": [
        "Charles University, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics (UFAL)"
    ],
    "DiscHierarchy": [
        "1.4",
        "Humanities",
        "Linguistics"
    ],
    "Discipline": "Linguistics",
    "Format": [
        "text/plain; charset=utf-8",
        "application/x-gzip",
        "downloadable_files_count: 3",
        "text/plain"
    ],
    "Language": [
        "Hindi"
    ],
    "MetaDataAccess": "https://clarin-pl.eu/oai/request?verb=GetRecord&metadataPrefix=oai_dc&identifier=oai:lindat.mff.cuni.cz:11858/00-097C-0000-0023-6260-A",
    "MetadataAccess": [
        "oai:lindat.mff.cuni.cz:11858/00-097C-0000-0023-6260-A"
    ],
    "PID": "http://hdl.handle.net/11858/00-097C-0000-0023-6260-A",
    "PublicationTimestamp": "2014-03-21T11:59:59Z",
    "PublicationYear": [
        "2014"
    ],
    "Publisher": [
        "Charles University, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics (UFAL)"
    ],
    "RelatedIdentifier": [
        "http://hdl.handle.net/11858/00-097C-0000-0001-CC1E-B"
    ],
    "ResourceType": [
        "corpus"
    ],
    "Rights": [
        "Attribution-NonCommercial-ShareAlike 3.0 Unported (CC BY-NC-SA 3.0)",
        "http://creativecommons.org/licenses/by-nc-sa/3.0/",
        "PUB"
    ],
    "author": [
        "Rychl\u00fd, Pavel",
        "Zeman, Daniel",
        "Stra\u0148\u00e1k, Pavel",
        "Suchomel, V\u00edt",
        "Bojar, Ond\u0159ej",
        "Tamchyna, Ale\u0161",
        "Diatka, Vojt\u011bch"
    ],
    "fulltext": "oai:lindat.mff.cuni.cz:11858/00-097C-0000-0023-6260-A;2018-07-02T22:05:50Z;hdl_11858_00-097C-0000-0001-486F-D;hdl_11858_00-097C-0000-0001-4877-A;HindMonoCorp 0.5;Bojar, Ond\u0159ej;Diatka, Vojt\u011bch;Rychl\u00fd, Pavel;Stra\u0148\u00e1k, Pavel;Suchomel, V\u00edt;Tamchyna, Ale\u0161;Zeman, Daniel;corpus;Hindi monolingual corpus. It is based primarily on web crawls performed using various tools and at various times. Since the web is a living data source, we treat these crawls as completely separate sources, despite they may overlap. To estimate the magnitude of this overlap, we compared the total number of segments if we concatenate the individual sources (each source being deduplicated on its own) with the number of segments if we de-duplicate all sources to- gether. The difference is just around 1%, confirming, that various web crawls (or their subsequent processings) differ significantly.\r\n\r\nHindMonoCorp contains data from:\r\nHindi web texts, a monolingual corpus containing mainly Hindi news articles has already been collected and released by Bojar et al. (2008). We use the HTML files as crawled for this corpus in 2010 and we add a small crawl performed in 2013 and re-process them with the current pipeline. These sources are denoted HWT 2010 and HWT 2013 in the following.\r\n\r\nHindi corpora in W2C have been collected by Martin Majli\u0161 during his project to automatically collect corpora in many languages (Majli\u0161 and \u017dabokrtsk\u00fd, 2012). There are in fact two corpora of Hindi available\u2014one from web harvest (W2C Web) and one from the Wikipedia (W2C Wiki).\r\n\r\nSpiderLing is a web crawl carried out during November and December 2013 using SpiderLing (Suchomel and Pomik\u00e1lek, 2012). The pipeline includes extraction of plain texts and deduplication at the level of documents, see below.\r\n\r\nCommonCrawl is a non-profit organization that regu- larly crawls the web and provides anyone with the data. We are grateful to Christian Buck for extracting plain text Hindi segments from the 2012 and 2013-fall crawls for us.\r\n\r\nIntercorp \u2013 7 books with their translations scanned and manually alligned per paragraph\r\n\r\nRSS Feeds from Webdunia.com and the Hindi version of BBC International followed by our custom crawler from September 2013 till January 2014.;2014-03-21;corpus;http://hdl.handle.net/11858/00-097C-0000-0023-6260-A;hin;http://hdl.handle.net/11858/00-097C-0000-0001-CC1E-B;Attribution-NonCommercial-ShareAlike 3.0 Unported (CC BY-NC-SA 3.0);http://creativecommons.org/licenses/by-nc-sa/3.0/;PUB;text/plain;application/x-gzip;application/x-gzip;text/plain; charset=utf-8;downloadable_files_count: 3;Charles University, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics (UFAL)",
    "group": "clarin",
    "groups": [
        {
            "name": "clarin"
        }
    ],
    "name": "f2481b9c-e219-5078-bba8-86ce74c0a94f",
    "notes": [
        "Hindi monolingual corpus. It is based primarily on web crawls performed using various tools and at various times. Since the web is a living data source, we treat these crawls as completely separate sources, despite they may overlap. To estimate the magnitude of this overlap, we compared the total number of segments if we concatenate the individual sources (each source being deduplicated on its own) with the number of segments if we de-duplicate all sources to- gether. The difference is just around 1%, confirming, that various web crawls (or their subsequent processings) differ significantly.\r\n\r\nHindMonoCorp contains data from:\r\nHindi web texts, a monolingual corpus containing mainly Hindi news articles has already been collected and released by Bojar et al. (2008). We use the HTML files as crawled for this corpus in 2010 and we add a small crawl performed in 2013 and re-process them with the current pipeline. These sources are denoted HWT 2010 and HWT 2013 in the following.\r\n\r\nHindi corpora in W2C have been collected by Martin Majli\u0161 during his project to automatically collect corpora in many languages (Majli\u0161 and \u017dabokrtsk\u00fd, 2012). There are in fact two corpora of Hindi available\u2014one from web harvest (W2C Web) and one from the Wikipedia (W2C Wiki).\r\n\r\nSpiderLing is a web crawl carried out during November and December 2013 using SpiderLing (Suchomel and Pomik\u00e1lek, 2012). The pipeline includes extraction of plain texts and deduplication at the level of documents, see below.\r\n\r\nCommonCrawl is a non-profit organization that regu- larly crawls the web and provides anyone with the data. We are grateful to Christian Buck for extracting plain text Hindi segments from the 2012 and 2013-fall crawls for us.\r\n\r\nIntercorp \u2013 7 books with their translations scanned and manually alligned per paragraph\r\n\r\nRSS Feeds from Webdunia.com and the Hindi version of BBC International followed by our custom crawler from September 2013 till January 2014."
    ],
    "oai_identifier": [
        "oai:lindat.mff.cuni.cz:11858/00-097C-0000-0023-6260-A"
    ],
    "oai_set": [
        "hdl_11858_00-097C-0000-0001-486F-D",
        "hdl_11858_00-097C-0000-0001-4877-A"
    ],
    "state": "active",
    "tags": [
        {
            "name": "corpus"
        }
    ],
    "title": [
        "HindMonoCorp 0.5"
    ],
    "url": ""
}