{
    "Contact": [
        "Masaryk University, NLP Centre"
    ],
    "DiscHierarchy": [
        "1.4",
        "Humanities",
        "Linguistics"
    ],
    "Discipline": "Linguistics",
    "Format": [
        "text/plain; charset=utf-8",
        "application/x-gzip",
        "downloadable_files_count: 1"
    ],
    "Language": [
        "English"
    ],
    "MetaDataAccess": "https://clarin-pl.eu/oai/request?verb=GetRecord&metadataPrefix=oai_dc&identifier=oai:lindat.mff.cuni.cz:11858/00-097C-0000-000D-F67B-7",
    "MetadataAccess": [
        "oai:lindat.mff.cuni.cz:11858/00-097C-0000-000D-F67B-7"
    ],
    "PID": "http://hdl.handle.net/11858/00-097C-0000-000D-F67B-7",
    "PublicationTimestamp": "2011-07-01T11:59:59Z",
    "PublicationYear": [
        "2011"
    ],
    "Publisher": [
        "Masaryk University, NLP Centre"
    ],
    "ResourceType": [
        "toolService"
    ],
    "Rights": [
        "BSD 3-Clause \"New\" or \"Revised\" license",
        "http://opensource.org/licenses/BSD-3-Clause",
        "PUB"
    ],
    "author": [
        "Pomik\u00e1lek, Jan"
    ],
    "fulltext": "oai:lindat.mff.cuni.cz:11858/00-097C-0000-000D-F67B-7;2018-07-02T22:05:49Z;hdl_11858_00-097C-0000-0001-486F-D;hdl_11858_00-097C-0000-0001-4877-A;onion;Pomik\u00e1lek, Jan;deduplication;corpus;text deduplication;n-gram deduplication;n-gram model;onion (ONe Instance ONly) is a tool for removing duplicate parts from large collections of texts. The tool has been implemented in Python, licensed under New BSD License and made an open source software (available for download including the source code at http://code.google.com/p/onion/). It is being successfuly used for cleaning large textual corpora at Natural language processing centre at Faculty of informatics, Masaryk university Brno and it's industry partners. The research leading to this piece of software was published in author's Ph.D. thesis \"Removing Boilerplate and Duplicate Content from Web Corpora\". The deduplication algorithm is based on comparing n-grams of words of text. The author's algorithm has been shown to be more suitable for textual corpora deduplication than competing algorithms (Broder, Charikar): in addition to detection of identical or very similar (95 %) duplicates, it is able to detect even partially similar duplicates (50 %) still achieving great performace (further described in author's Ph.D. thesis). The unique deduplication capabilities and scalability of the algorithm were been demonstrated while building corpora of American Spanish, Arabic, Czech, French, Japanese, Russian, Tajik, and six Turkic languages consisting --- several TB of text documents were deduplicated resulting in corpora of 70 billions tokens altogether.;2011;toolService;http://hdl.handle.net/11858/00-097C-0000-000D-F67B-7;eng;BSD 3-Clause \"New\" or \"Revised\" license;http://opensource.org/licenses/BSD-3-Clause;PUB;application/x-gzip;text/plain; charset=utf-8;downloadable_files_count: 1;Masaryk University, NLP Centre;http://code.google.com/p/onion/",
    "group": "clarin",
    "groups": [
        {
            "name": "clarin"
        }
    ],
    "name": "d915a964-f24d-5b34-8fe9-bb9327a18aa1",
    "notes": [
        "onion (ONe Instance ONly) is a tool for removing duplicate parts from large collections of texts. The tool has been implemented in Python, licensed under New BSD License and made an open source software (available for download including the source code at http://code.google.com/p/onion/). It is being successfuly used for cleaning large textual corpora at Natural language processing centre at Faculty of informatics, Masaryk university Brno and it's industry partners. The research leading to this piece of software was published in author's Ph.D. thesis \"Removing Boilerplate and Duplicate Content from Web Corpora\". The deduplication algorithm is based on comparing n-grams of words of text. The author's algorithm has been shown to be more suitable for textual corpora deduplication than competing algorithms (Broder, Charikar): in addition to detection of identical or very similar (95 %) duplicates, it is able to detect even partially similar duplicates (50 %) still achieving great performace (further described in author's Ph.D. thesis). The unique deduplication capabilities and scalability of the algorithm were been demonstrated while building corpora of American Spanish, Arabic, Czech, French, Japanese, Russian, Tajik, and six Turkic languages consisting --- several TB of text documents were deduplicated resulting in corpora of 70 billions tokens altogether."
    ],
    "oai_identifier": [
        "oai:lindat.mff.cuni.cz:11858/00-097C-0000-000D-F67B-7"
    ],
    "oai_set": [
        "hdl_11858_00-097C-0000-0001-486F-D",
        "hdl_11858_00-097C-0000-0001-4877-A"
    ],
    "state": "active",
    "tags": [
        {
            "name": "deduplication"
        },
        {
            "name": "corpus"
        },
        {
            "name": "text deduplication"
        },
        {
            "name": "n-gram deduplication"
        },
        {
            "name": "n-gram model"
        }
    ],
    "title": [
        "onion"
    ],
    "url": ""
}